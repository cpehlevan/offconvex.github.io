---
layout: post
date:  2019-01-06 10:45:00
title: The search for biologically plausible neural computation&#58; Non-negative similarity-based networks for clustering and manifold tiling
author: Cengiz Pehlevan and Dmitri "Mitya" Chklovskii
visible: True
---

In the [previous post](http://www.offconvex.org/2018/12/03/MityaNN2/) of this series, we introduced novel biologically-plausible neural networks (NNs) the operation of which, including both neural activity dynamics and local synaptic learning rules, is derived by optimizing a similarity-based objective. When the output of such NNs is constrained to be non-negative as suggested by biology, they empirically solve interesting tasks such as clustering and manifold learning. But finding the optimal solution for a constrained similarity-based objective is often challenging similarly to that for the non-negative matrix factorization problem.

 Here, we introduce a simplified similarity-based objective that allows us to make progress with the analysis and admits an intuitive interpretation. First, we address the simpler clustering task which, for highly segregated data, has a straightforward optimal solution. Second, we address manifold learning by viewing it as a soft-clustering problem. Finally, we discuss evidence from the brain and remaining open questions.

## A similarity-based cost function and NN for clustering

The key to our analysis is formulating a similarity-based cost function optimization of which will yield an online algorithm and a NN for clustering. Let ${\bf x}_{t=1,...,T}\in \mathbb{R}^n$ be a set of data points (inputs). We define the similarity of a pair of inputs, ${\bf x}_t$ and ${\bf x} _{t'}$, as their dot-product, ${\bf x}_t^\top {\bf x} _{t'}$. We would like to assign inputs to $k$ clusters based on pairwise similarities, so that the algorithm outputs cluster assignment indices ${\bf y} _{t=1,...,T} \in \mathbb{R}^k$.

To arrive at a cost function, consider first a single pair of data points, ${\bf x}_1$ and ${\bf x}_2$. If their similarity, ${\bf x} _1^\top {\bf x} _{2} < \alpha$, where $\alpha$ is a pre-set threshold, then the points should be assigned to separate clusters, i.e. ${\bf y} _1 = [1,0]^\top$ and  ${\bf y} _2 = [0,1]^\top$, setting output similarity, ${\bf y} _1^\top{\bf y} _{2}$ to 0. If ${\bf x} _1^\top {\bf x} _{2}>\alpha$, then the points are assigned to the same cluster, e.g. ${\bf y} _1={\bf y} _{2} = [1,0]^\top$. Such ${\bf y} _1$ and ${\bf y} _2$ are optimal solutions (although not unique) to the following optimization problem:

$$\min_{ {\bf y} _1\geq 0,{\bf y} _2\geq 0 } \left(\alpha-{\bf x} _1^\top {\bf x}_2\right) {\bf y} _1^\top {\bf y} _{2}, \qquad {\rm s.t.} \quad \left\Vert{\bf y} _1\right\Vert _2 \leq 1, \, \left\Vert{\bf y} _{2}\right\Vert _2 \leq 1, \quad (3.1)$$

where, in abuse of notation, ${\bf y} _t\geq 0 $  means that every component of the vector ${\bf y} _t$ is nonnegative.

To obtain an objective function that would cluster the whole dataset of $T$ inputs we simply sum (3.1) over all possible input pairs:

$$\min_{ {\bf y} _1\geq 0,...,{\bf y} _T\geq 0 } \sum_{t=1}^T\sum_{t'=1}^T\left(\alpha-{\bf x} _t^\top {\bf x} _{t'}\right){\bf y} _t^\top{\bf y} _{t'} \qquad \qquad  \qquad\\
{\rm s.t.}\quad \left\Vert{\bf y} _1\right\Vert _2\leq 1, \quad\ldots\quad ,\left\Vert{\bf y} _T\right\Vert _2\leq 1.\qquad \qquad (3.2)$$

Does optimization of (3.2) produce the desired clustering output? This depends on the dataset. If a threshold, $\alpha$, exists such that the similarities of all pairs within the same cluster are greater and similarities of pairs from different clusters are less than $\alpha$, then the cost function (3.2) is minimized by the desired hard-clustering output, provided that $k$ is greater than or equal to the number of clusters (Figure 1A).

To solve the objective (3.2) in the online setting, we introduce the constraints in the cost via Lagrange multipliers and using the variable substitution trick from the previous blog post in the series, [we can derive a NN implementation of this algorithm](http://papers.nips.cc/paper/7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks) (Figure 1B).

![](https://drive.google.com/uc?export=view&id=1PdNPaWT8Ikdo6uJ_cUOIB6LtOaoCjXEX)

*Figure 1:  A) An artificial dataset and corresponding optimal output vectors. Points are colored and grouped with respect to assigned clusters.  B) A biologically-plausible excitatory-inhibitory NN implementation of the algorithm. [In this version](http://papers.nips.cc/paper/7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks), anti-Hebbian synapses operate at a faster time scale than Hebbian synapses.*

## Manifold-tiling solutions

In many real-world problems, data points are not well-segregated but lie on low-dimensional manifolds. For such data, [the optimal solution of the objective (3.2) effectively tiles the  data manifold](http://papers.nips.cc/paper/7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks).

We can understand such optimal solutions using soft-clustering, i.e. clustering where each stimulus may be assigned to more than one cluster and assignment indices are real numbers between zero and one. Each output neuron is characterized by the weight vector of incoming synapses which defines a centroid in the input data space. The response of a neuron is maximum when data fall on the centroid and decays away from it. Manifold-tiling solutions for several datasets are shown in Figure 2.

![](https://drive.google.com/uc?export=view&id=1eWZ-63hxp1yiSVqCa4qnJ20Uspi5FVxi)

*Figure 2:  [Analytical and numerical manifold-tiling solutions of (3.2) for representative datasets provide accurate and useful representations.](http://papers.nips.cc/paper/7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks) A) A circular manifold (left) is tiled by  overlapping localized receptive fields (right). In the continuum limit ($k\rightarrow \infty$), receptive fields are truncated cosines of the polar angle, $\theta$. B) Similar analytical and numerical results are obtained for a spherical 3D manifold. We show a few receptive fields in different colors over three different views of the spherical manifold.  C) Learning the manifold of the 0 digit from the MNIST dataset by tiling the manifold with
overlapping localized receptive fields. Left: Two-dimensional linear embedding (PCA) of the outputs. The data gets organized according to different visual characteristics
of the hand-written digit (e.g., orientation and elongation). Right: A few receptive fields in different colors over the low-dimensional embedding.*


We can prove this result analytically by taking advantage of the convex formulation in the limit of infinite $k$. Indeed, if we introduce Gramians ${\bf D}$, such that $D _{tt'}={\bf x} _t^\top {\bf x} _{t'}$, and ${\bf Q}$, such that $Q _{tt'}= {\bf y} _t^\top {\bf y} _{t'}$ and do not specify the dimensionality of ${\bf y}$ by leaving the rank of ${\bf Q}$ open, we can rewrite (3.2) as:

$$ \min_{ {\bf Q}\in \mathcal{CP}^T ,\, {\rm diag}{\bf Q} \le{\bf 1} }
 -{\rm Tr}(({\bf D}-\alpha {\bf E}){\bf Q}). \qquad (3.3)$$

 where a $T\times T$ matrix belongs to a [*completely positive*](https://books.google.com/books/about/Completely_Positive_Matrices.html?id=d3JpDQAAQBAJ) cone denoted by $\mathcal{CP}^T$, i.e. ${\bf Q}\equiv{\bf Y}^\top{\bf Y}$ with ${\bf Y}\ge 0$.  Redefining the variables makes the optimization problem convex. For arbitrary datasets, [optimization problems in $\mathcal{CP}^T$ are often intractable for large $T$](https://books.google.com/books/about/Completely_Positive_Matrices.html?id=d3JpDQAAQBAJ), despite the convexity. [However, for symmetric datasets, i.e. circle, 2-sphere and SO(3), we can optimize (3.3) by analyzing the  Karush–Kuhn–Tucker conditions](http://papers.nips.cc/paper/7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks) (Figure 2).

## Other similarity-based NN approaches to clustering and manifold-tiling

 While we chose to present our similarity-based NN approach through the cost function in (3.2), similar results can be obtained for other versions of similarity-based clustering objective functions. The nonnegative similarity-matching cost function Eq. (2.10) introduced in the [previous post](http://www.offconvex.org/2018/12/03/MityaNN2/) of this series, and the NN derived from it (Figure 3A) can be used for clustering (see [here](https://www.cc.gatech.edu/~hpark/papers/DaDingParkSDM12.pdf) and [here](https://arxiv.org/abs/1503.00680)) and [manifold learning](http://papers.nips.cc/paper/7939-manifold-tiling-localized-receptive-fields-are-optimal-in-similarity-preserving-neural-networks) as well. [The $k$-means cost function can be cast into a similarity-based form and an NN (Figure 3B) can be derived for its online implementation](https://www.biorxiv.org/content/early/2018/01/27/226746). Finally, all these NNs here can be augmented by an initial [random, nonlinear projection layer](https://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning) to [perform clustering in the feature space associated with certain kernels](https://www.researchgate.net/publication/320165071_Neural_Networks_for_Efficient_Nonlinear_Online_Clustering)  (Figure 3C).

 Objective (3.2) is related to the previously studied convex relaxation of community detection in graphs (see [here](https://arxiv.org/abs/1406.5647) and [here](https://arxiv.org/abs/1404.6000)), which is closely related to clustering. If, instead of the nonnegativity of ${\bf Y}$ we require the nonnegativity of ${\bf Q}$ we arrive at an semidefinite programming formulation:

 $$ \min_{ {\bf Q} \succeq 0,  \, {\bf Q}\geq 0, \, {\rm diag}{\bf Q} \le{\bf 1} } -{\rm Tr} (({\bf D}-\alpha {\bf E}){\bf Q}). \qquad\quad (3.4)$$

 Finally, we introduced a soft-$k$-means cost, also a relaxation of another semidefinite program for clustering (see [here](http://www.cs.utexas.edu/users/ai-lab/pubs/kernel-kdd-05.pdf) and [here](https://arxiv.org/abs/1408.4045)), and an [associated NN](https://www.biorxiv.org/content/early/2018/01/27/226746) (Figure 3B), and [showed that they can perform manifold tiling](http://jmlr.org/papers/volume19/18-088/18-088.pdf).

 ![](https://drive.google.com/uc?export=view&id=1vZdXpUY8eOcCRaLd_bBdol0tEUH-XV4H)

 *Figure 3:  Other biologically-plausible NNs for clustering and manifold learning. A) Nonnegative similarity matching network, also introduced in the previous blog post. B) Hard and soft $k$-means networks. Rectified neurons are perfect (hard $k$-means) or leaky (soft $k$-means) integrators. They have learned (homeostatic) activation thresholds and ephaptic couplings. C) When augmented with a hidden nonlinear layer, the presented networks perform clustering in the nonlinear feature space. Shown is the NN given [here](https://www.researchgate.net/publication/320165071_Neural_Networks_for_Efficient_Nonlinear_Online_Clustering), where the hidden layer is formed of [Random Fourier Features](https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines) to obtain a low-rank approximation to a Gaussian kernel. Hence, the whole two-layer NN operates as an online kernel clustering algorithm.*

## Manifold-tiling and clustering in the brain

 Similarity-based networks were originally introduced to model brain function. Indeed, clustering and manifold learning are encountered in the brain. Although sensory stimuli are nominally very high-dimensional (up to $\sim 10^{5-6}$) natural stimuli often populate low-dimensional manifolds. Learning such manifolds is essential for avoiding the curse of dimensionality. In many brain areas, neurons collectively tile the stimulus manifold, each responding only to a small neighborhood of the stimulus space.  For example, a hippocampus place cell is [active in a particular spatial location](http://www.cognitivemap.net/HCMpdf/HCMComplete.pdf)  (Figure 4), the response of a V1 neuron is [localized in visual space and orientation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/pdf/jphysiol01247-0121.pdf), and the response of an auditory neuron is [localized in the sound frequency space](http://science.sciencemag.org/content/202/4369/778.long). Further, all these responses are learned in an activity-dependent manner and can be shaped by varying stimulus statistics and perturbing neural activity.

Olfactory object recognition in insects [can be modeled by a clustering network](https://www.biorxiv.org/content/early/2018/01/27/226746), (Figure 3B).


![](https://upload.wikimedia.org/wikipedia/commons/5/5e/Place_Cell_Spiking_Activity_Example.png)

*Figure 4:  Illustration of place cells from [Wikipedia](https://en.wikipedia.org/wiki/Place_cell): ``Spatial firing patterns of 8 place cells recorded from the CA1 layer of a rat. The rat ran back and forth along an elevated track, stopping at each end to eat a small food reward. Dots indicate positions where action potentials were recorded, with color indicating which neuron emitted that action potential."*

## Remaining challenges

As discussed in this and previous posts of the series, similarity-based NNs can solve unsupervised tasks such as dimensionality reduction, clustering and manifold learning in a biologically plausible way. Yet, despite these successes of similarity-based NNs, many interesting challenges remain. Next, we list a few of them.

Whereas numerical experiments indicate that our online algorithms perform well, most of them lack  global convergence proofs. Even for PCA networks we can only prove linear stability of the desired solution in the stochastic approximation setting.

Motivated by biological learning which is mostly unsupervised we focused on unsupervised learning. Yet, supervision or reinforcement takes place in the brain. Therefore, it is desirable to extend our framework to supervised, semi-supervised and reinforcement learning settings. Naturally, such extensions may be valuable also as general purpose machine learning algorithms.

Whereas most sensory stimuli are correlated time series, we assumed that data points at different times are independent. How are temporal correlations analyzed by NNs? Solving this problem is important both for modeling brain function and developing general purpose machine learning algorithms.

Another challenge is stacking similarity-based NNs. [Heuristic approach to stacking yields promising results](https://arxiv.org/abs/1702.06456). Yet, except for the Nonnegative ICA problem introduced in the [previous post](http://www.offconvex.org/2018/12/03/MityaNN2/), we do not have a theoretical understanding of how and why to stack similarity-based NNs.

Finally, neurons in biological NNs signal each other using all-or-none spikes, or action potentials, as opposed to real-valued signals we considered. Is there a  normative theory accounting for spiking in biological NNs?
