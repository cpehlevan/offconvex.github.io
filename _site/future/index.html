<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Off the convex path</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Algorithms off the convex path.">
    <meta name="author" content="Moritz Hardt">
    
    <link rel="canonical" href="http://localhost:4000/future/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Off the convex path" href="/feed.xml" />
    <link rel="stylesheet" href="/css/pixyll.css?201901062011" type="text/css">

    <!-- Fonts -->
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Off the convex path">
    <meta property="og:description" content="Algorithms off the convex path.">
    <meta property="og:url" content="http://localhost:4000/future/">
    <meta property="og:site_name" content="Off the convex path">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
          processEscapes: true
        },
        messageStyle: "none",
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
    </script>

</head>

<body class="site">

	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://localhost:4000" class="site-title">
      <img style="width:500px;" src="/assets/logo.jpg" />
      </a>
      <nav class="site-nav" style="padding-top:200px;">
        <a href="/about/">About</a>
<a href="/contact/">Contact</a>
<a href="/subscribe/">Subscribe</a>

      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <div class="home">

  <div class="posts">
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2019/01/06/MityaNN3/" class="post-link">
        The search for biologically plausible neural computation&#58; Non-negative similarity-based networks for clustering and manifold tiling
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jan 6, 2019.&nbsp;&nbsp;</span>
In the previous post of this series, we introduced novel biologically-plausible neural networks (NNs) the operation of which, including both neural activity dynamics and local synaptic learning rules, is derived by optimizing a similarity-based objective.... <a href="/2019/01/06/MityaNN3/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/12/03/MityaNN2/" class="post-link">
        The search for biologically plausible neural computation&#58; A similarity-based approach
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 3, 2018.&nbsp;&nbsp;</span>
This is the second post in a series reviewing recent progress in designing artificial neural networks (NNs) that resemble natural NNs not just superficially, but on a deeper, algorithmic level. In addition to serving as... <a href="/2018/12/03/MityaNN2/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/11/07/optimization-beyond-landscape/" class="post-link">
        Understanding optimization in deep learning by analyzing trajectories of gradient descent
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Nov 7, 2018.&nbsp;&nbsp;</span>
Neural network optimization is fundamentally non-convex, and yet simple gradient-based algorithms seem to consistently solve such problems. This phenomenon is one of the central pillars of deep learning, and forms a mystery many of us... <a href="/2018/11/07/optimization-beyond-landscape/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/09/18/alacarte/" class="post-link">
        Simple and efficient semantic embeddings for rare words, n-grams, and language features
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Sep 18, 2018.&nbsp;&nbsp;</span>
Distributional methods for capturing meaning, such as word embeddings, often require observing many examples of words in context. But most humans can infer a reasonable meaning from very few or even a single occurrence. For... <a href="/2018/09/18/alacarte/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/07/27/approximating-recurrent/" class="post-link">
        When Recurrent Models Don't Need to be Recurrent
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jul 27, 2018.&nbsp;&nbsp;</span>
In the last few years, deep learning practitioners have proposed a litany of different sequence models. Although recurrent neural networks were once the tool of choice, now models like the autoregressive Wavenet or the Transformer... <a href="/2018/07/27/approximating-recurrent/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/06/25/textembeddings/" class="post-link">
        Deep-learning-free Text and Sentence Embedding, Part 2
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jun 25, 2018.&nbsp;&nbsp;</span>
This post continues Sanjeev’s post and describes further attempts to construct elementary and interpretable text embeddings. The previous post described the the SIF embedding, which uses a simple weighted combination of word embeddings combined with... <a href="/2018/06/25/textembeddings/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/06/17/textembeddings/" class="post-link">
        Deep-learning-free Text and Sentence Embedding, Part 1
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jun 17, 2018.&nbsp;&nbsp;</span>
Word embeddings (see my old post1 and post2) capture the idea that one can express “meaning” of words using a vector, so that the cosine of the angle between the vectors captures semantic similarity. (“Cosine... <a href="/2018/06/17/textembeddings/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/03/12/bigan/" class="post-link">
        Limitations of Encoder-Decoder GAN architectures
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 12, 2018.&nbsp;&nbsp;</span>
This is yet another post about Generative Adversarial Nets (GANs), and based upon our new ICLR’18 paper with Yi Zhang. A quick recap of the story so far. GANs are an unsupervised method in deep... <a href="/2018/03/12/bigan/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/03/02/acceleration-overparameterization/" class="post-link">
        Can increasing depth serve to accelerate optimization?
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 2, 2018.&nbsp;&nbsp;</span>
“How does depth help?” is a fundamental question in the theory of deep learning. Conventional wisdom, backed by theoretical studies (e.g. Eldan &amp; Shamir 2016; Raghu et al. 2017; Lee et al. 2017; Cohen et... <a href="/2018/03/02/acceleration-overparameterization/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2018/02/17/generalization2/" class="post-link">
        Proving generalization of deep nets via compression
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Feb 17, 2018.&nbsp;&nbsp;</span>
This post is about my new paper with Rong Ge, Behnam Neyshabur, and Yi Zhang which offers some new perspective into the generalization mystery for deep nets discussed in my earlier post. The new paper... <a href="/2018/02/17/generalization2/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/12/08/generalization1/" class="post-link">
        Generalization Theory and Deep Nets, An introduction
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 8, 2017.&nbsp;&nbsp;</span>
Deep learning holds many mysteries for theory, as we have discussed on this blog. Lately many ML theorists have become interested in the generalization mystery: why do trained deep nets perform well on previously unseen... <a href="/2017/12/08/generalization1/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/07/19/saddle-efficiency/" class="post-link">
        How to Escape Saddle Points Efficiently
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jul 19, 2017.&nbsp;&nbsp;</span>
A core, emerging problem in nonconvex optimization involves the escape of saddle points. While recent research has shown that gradient descent (GD) generically escapes saddle points asymptotically (see Rong Ge’s and Ben Recht’s blog posts),... <a href="/2017/07/19/saddle-efficiency/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/07/06/GANs3/" class="post-link">
        Do GANs actually do distribution learning?
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jul 6, 2017.&nbsp;&nbsp;</span>
This post is about our new paper, which presents empirical evidence that current GANs (Generative Adversarial Nets) are quite far from learning the target distribution. Previous posts had introduced GANs and described new theoretical analysis... <a href="/2017/07/06/GANs3/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/06/26/unsupervised1/" class="post-link">
        Unsupervised learning, one notion or many?
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jun 26, 2017.&nbsp;&nbsp;</span>
Unsupervised learning, as the name suggests, is the science of learning from unlabeled data. A look at the wikipedia page shows that this term has many interpretations: (Task A) Learning a distribution from samples. (Examples:... <a href="/2017/06/26/unsupervised1/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/03/30/GANs2/" class="post-link">
        Generalization and Equilibrium in Generative Adversarial Networks (GANs)
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 30, 2017.&nbsp;&nbsp;</span>
The previous post described Generative Adversarial Networks (GANs), a technique for training generative models for image distributions (and other complicated distributions) via a 2-party game between a generator deep net and a discriminator deep net.... <a href="/2017/03/30/GANs2/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2017/03/15/GANs/" class="post-link">
        Generative Adversarial Networks (GANs), Some Open Questions
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 15, 2017.&nbsp;&nbsp;</span>
Since ability to generate “realistic-looking” data may be a step towards understanding its structure and exploiting it, generative models are an important component of unsupervised learning, which has been a frequent theme on this blog.... <a href="/2017/03/15/GANs/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/12/20/backprop/" class="post-link">
        Back-propagation, an introduction
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 20, 2016.&nbsp;&nbsp;</span>
Given the sheer number of backpropagation tutorials on the internet, is there really need for another? One of us (Sanjeev) recently taught backpropagation in undergrad AI and couldn’t find any account he was happy with.... <a href="/2016/12/20/backprop/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/11/03/MityaNN1/" class="post-link">
        The search for biologically plausible neural computation&#58; The conventional approach
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Nov 3, 2016.&nbsp;&nbsp;</span>
Inventors of the original artificial neural networks (NNs) derived their inspiration from biology. However, as artificial NNs progressed, their design was less guided by neuroscience facts. Meanwhile, progress in neuroscience has altered our conceptual understanding... <a href="/2016/11/03/MityaNN1/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/10/13/gradient-descent-learns-dynamical-systems/" class="post-link">
        Gradient Descent Learns Linear Dynamical Systems
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Oct 13, 2016.&nbsp;&nbsp;</span>
From text translation to video captioning, learning to map one sequence to another is an increasingly active research area in machine learning. Fueled by the success of recurrent neural networks in its many variants, the... <a href="/2016/10/13/gradient-descent-learns-dynamical-systems/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/07/10/embeddingspolysemy/" class="post-link">
        Linear algebraic structure of word meanings
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jul 10, 2016.&nbsp;&nbsp;</span>
Word embeddings capture the meaning of a word using a low-dimensional vector and are ubiquitous in natural language processing (NLP). (See my earlier post 1 and post2.) It has always been unclear how to interpret... <a href="/2016/07/10/embeddingspolysemy/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/05/08/almostconvexitySATM/" class="post-link">
        A Framework for analysing Non-Convex Optimization
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">May 8, 2016.&nbsp;&nbsp;</span>
Previously Rong’s post and Ben’s post show that (noisy) gradient descent can converge to local minimum of a non-convex function, and in (large) polynomial time (Ge et al.’15). This post describes a simple framework that... <a href="/2016/05/08/almostconvexitySATM/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/04/04/markov-chains-dynamical-systems/" class="post-link">
        Markov Chains Through the Lens of Dynamical Systems&#58; The Case of Evolution
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Apr 4, 2016.&nbsp;&nbsp;</span>
In this post, we will see the main technical ideas in the analysis of the mixing time of evolutionary Markov chains introduced in a previous post. We start by introducing the notion of the expected... <a href="/2016/04/04/markov-chains-dynamical-systems/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/03/24/saddles-again/" class="post-link">
        Saddles Again
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 24, 2016.&nbsp;&nbsp;</span>
Thanks to Rong for the very nice blog post describing critical points of nonconvex functions and how to avoid them. I’d like to follow up on his post to highlight a fact that is not... <a href="/2016/03/24/saddles-again/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/03/22/saddlepoints/" class="post-link">
        Escaping from Saddle Points
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 22, 2016.&nbsp;&nbsp;</span>
Convex functions are simple — they usually have only one local minimum. Non-convex functions can be much more complicated. In this post we will discuss various types of critical points that you might encounter when... <a href="/2016/03/22/saddlepoints/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/03/14/stability/" class="post-link">
        Stability as a foundation of machine learning
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 14, 2016.&nbsp;&nbsp;</span>
Central to machine learning is our ability to relate how a learning algorithm fares on a sample to its performance on unseen instances. This is called generalization. In this post, I will describe a purely... <a href="/2016/03/14/stability/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/03/07/evolution-markov-chains/" class="post-link">
        Evolution, Dynamical Systems and Markov Chains
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Mar 7, 2016.&nbsp;&nbsp;</span>
In this post we present a high level introduction to evolution and to how we can use mathematical tools such as dynamical systems and Markov chains to model it. Questions about evolution then translate to... <a href="/2016/03/07/evolution-markov-chains/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/02/14/word-embeddings-2/" class="post-link">
        Word Embeddings&#58; Explaining their properties
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Feb 14, 2016.&nbsp;&nbsp;</span>
This is a followup to an earlier post about word embeddings, which capture the meaning of a word using a low-dimensional vector, and are ubiquitous in natural language processing. I will talk about my joint... <a href="/2016/02/14/word-embeddings-2/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2016/01/25/non-convex-workshop/" class="post-link">
        NIPS 2015 workshop on non-convex optimization
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Jan 25, 2016.&nbsp;&nbsp;</span>
While convex analysis has received much attention by the machine learning community, theoretical analysis of non-convex optimization is still nascent. This blog as well as the recent NIPS 2015 workshop on non-convex optimization aim to... <a href="/2016/01/25/non-convex-workshop/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2015/12/21/dynamical-systems-1/" class="post-link">
        Nature, Dynamical Systems and Optimization
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 21, 2015.&nbsp;&nbsp;</span>
The language of dynamical systems is the preferred choice of scientists to model a wide variety of phenomena in nature. The reason is that, often, it is easy to locally observe or understand what happens... <a href="/2015/12/21/dynamical-systems-1/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2015/12/17/tensor-decompositions/" class="post-link">
        Tensor Methods in Machine Learning
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 17, 2015.&nbsp;&nbsp;</span>
Tensors are high dimensional generalizations of matrices. In recent years tensor decompositions were used to design learning algorithms for estimating parameters of latent variable models like Hidden Markov Model, Mixture of Gaussians and Latent Dirichlet... <a href="/2015/12/17/tensor-decompositions/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2015/12/12/word-embeddings-1/" class="post-link">
        Semantic Word Embeddings
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 12, 2015.&nbsp;&nbsp;</span>
This post can be seen as an introduction to how nonconvex problems arise naturally in practice, and also the relative ease with which they are often solved. I will talk about word embeddings, a geometric... <a href="/2015/12/12/word-embeddings-1/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2015/12/11/mission-statement/" class="post-link">
        Why go off the convex path?
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 11, 2015.&nbsp;&nbsp;</span>
The notion of convexity underlies a lot of beautiful mathematics. When combined with computation, it gives rise to the area of convex optimization that has had a huge impact on understanding and improving the world... <a href="/2015/12/11/mission-statement/">Continue</a>
        </p>
      </div>
    
      <div class="post">
        
    <h3 class="h2 post-title">
      <a href="/2015/12/01/template/" class="post-link">
        Title of your post
      </a>
    </h3>
        <p class="post-summary">
        <span class="post-meta">Dec 1, 2015.&nbsp;&nbsp;</span>
Enter content here in markdown format. See guidelines. When you’re
done and you want the post to appear publicly, change the last line of the meta
data to:

visible:    true

 <a href="/2015/12/01/template/">Continue</a>
        </p>
      </div>
    
  </div>

  <div class="pagination clearfix mb1 mt4">
  <div class="left">
    
      <span class="pagination-item disabled">Newer</span>
    
  </div>
  <div class="right">
    
      <span class="pagination-item disabled">Older</span>
    
  </div>
</div>

</div>

      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Theme available on <a href="https://github.com/johnotander/pixyll">Github</a>.
    </small>
  </div>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-70478681-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

  
  
</body>
</html>
