<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Linear algebraic structure of word meanings &#8211; Off the convex path</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Understanding how polysemy relates to word embeddings, and use this to extract senses of a word.">
    <meta name="author" content="Moritz Hardt">
    <meta name="keywords" content="">
    <link rel="canonical" href="http://localhost:4000/2016/07/10/embeddingspolysemy/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for Off the convex path" href="/feed.xml" />
    <link rel="stylesheet" href="/css/pixyll.css?201812041448" type="text/css">

    <!-- Fonts -->
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Linear algebraic structure of word meanings">
    <meta property="og:description" content="Algorithms off the convex path.">
    <meta property="og:url" content="http://localhost:4000/2016/07/10/embeddingspolysemy/">
    <meta property="og:site_name" content="Off the convex path">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
          processEscapes: true
        },
        messageStyle: "none",
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js">
    </script>

</head>

<body class="site">

	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
      <a href="http://localhost:4000" class="site-title">
      <img style="width:500px;" src="/assets/logo.jpg" />
      </a>
      <nav class="site-nav" style="padding-top:200px;">
        <a href="/about/">About</a>
<a href="/contact/">Contact</a>
<a href="/subscribe/">Subscribe</a>

      </nav>
      <div class="clearfix"></div>
      
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        


<div class="post-header mb2">
  <h1>Linear algebraic structure of word meanings</h1>
  <div class="post-meta">
  
  Sanjeev Arora &nbsp;&bull;&nbsp;
  
  
  Jul 10, 2016 &nbsp;&bull;&nbsp;
  
  
  
    13 minute read
  
  </div>
</div>

<article class="post-content">
  <p>Word embeddings capture the meaning of a word using a low-dimensional vector and are ubiquitous in natural language processing (NLP). (See my  earlier <a href="http://www.offconvex.org/2015/12/12/word-embeddings-1/">post 1</a>
and <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">post2</a>.) It has always been unclear how to interpret the embedding when the word in question is <em>polysemous,</em> that is, has multiple senses. For example, <em>tie</em> can mean an article of clothing, a drawn sports match, and a physical action.</p>

<p>Polysemy is an important issue in NLP  and much work relies upon <a href="https://wordnet.princeton.edu/">WordNet</a>, a hand-constructed repository of word senses and their interrelationships. Unfortunately, good WordNets do not exist for most languages, and even the one in English  is believed to be rather incomplete. Thus some effort has been spent on methods to find different senses of words.</p>

<p>In this post I will talk about <a href="https://arxiv.org/abs/1601.03764">my joint work with Li, Liang, Ma, Risteski</a> which shows that actually word senses are easily accessible in many current word embeddings. This goes against conventional wisdom in NLP, which is that <em>of course</em>, word embeddings do not suffice to capture polysemy since they use a single vector to represent the word, regardless of whether the word has one sense, or a dozen.  Our work shows that major senses of the word lie in linear superposition within the embedding, and are extractable using sparse coding.</p>

<p>This post uses embeddings constructed using our method and the wikipedia corpus, but similar techniques also apply (with some loss in precision) to  other embeddings described in <a href="http://www.offconvex.org/2015/12/12/word-embeddings-1/">post 1</a> such as word2vec, Glove, or even the decades-old PMI embedding.</p>

<h2 id="a-surprising-experiment">A surprising experiment</h2>

<p>Take the viewpoint –simplistic yet instructive– that a polysemous word like <em>tie</em> is a single lexical token that represents unrelated words <em>tie1</em>, <em>tie2</em>, …
Here is a surprising experiment that suggests that the embedding for <em>tie</em> should be approximately a weighted sum of the (hypothethical) embeddings of <em>tie1</em>, <em>tie2</em>, …</p>

<blockquote>
  <p>Take two random  words $w_1, w_2$. Combine them into an artificial polysemous word $w_{new}$ by replacing every occurrence of $w_1$ or $w_2$ in the corpus by $w_{new}.$ Next, compute an embedding for $w_{new}$ using the same embedding method while deleting embeddings for $w_1, w_2$ but preserving the embeddings for all other words. Compare the embedding $v_{w_{new}}$ to linear combinations of $v_{w_1}$ and
$v_{w_2}$.</p>
</blockquote>

<p>Repeating this experiment with a wide range of values for the ratio $r$ between the frequencies of $w_1$ and $w_2$, we find that $v_{w_{new}}$ lies close to the subspace spanned by $v_{w_1}$ and $v_{w_2}$: the cosine of its angle with the subspace  is on average $0.97$ with standard deviation $0.02$. Thus  $v_{w_{new}} \approx \alpha v_{w_1} + \beta v_{w_2}$. 
We find that $\alpha \approx 1$ whereas  $\beta \approx 1- c\lg r$
 for some constant $c\approx 0.5$. (Note this formula is meaningful when the frequency ratio $r$ is not too large, i.e. when $ r &lt; 10^{1/c} \approx 100$.) Thanks to this logarithm, the infrequent sense is not swamped out in the embedding, even if it is 50 times less frequent than the dominant sense. This is an important reason behind the success of our method for extracting word senses.</p>

<p>This experiment –to which we were led by our theoretical investigations– is very surprising 
because the embedding is the solution to a complicated, nonconvex optimization, yet it behaves in such a striking linear way. You can read our paper for an intuitive explanation using our theoretical model from <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">post2</a>.</p>

<h2 id="extracting-word-senses-from-embeddings">Extracting word senses from embeddings</h2>

<p>The above experiment suggests that</p>

<script type="math/tex; mode=display">v_{tie} \approx \alpha_1 \cdot v_{ tie1} + \alpha_2 \cdot v_{tie2} + \alpha_3 \cdot v_{tie3} +\cdots \qquad (1)</script>

<p>but this alone is insufficient to mathematically pin down the senses, since $v_{tie}$ can be expressed in infinitely many ways as such a combination. To pin down the senses we will interrelate the senses of different words —for example, relate the “article of clothing” sense <em>tie1</em> with  <em>shoe, jacket</em> etc.</p>

<p>The word senses <em>tie1, tie2,..</em> correspond to “different things being talked about” —in other words, different word distributions occuring around  <em>tie</em>.
 Now remember that <a href="http://128.84.21.199/abs/1502.03520v6">our earlier paper</a> described in 
 <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">post2</a> gives an interpretation of “what’s being talked about”: it is called <em>discourse</em> and 
 it is represented by a unit vector in the embedding space. In particular, the theoretical model 
 of <a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">post2</a> imagines a text corpus as being generated by a random walk on 
 discourse vectors. When the walk is at a discourse $c_t$ at time $t$, it outputs a few words using a loglinear distribution:</p>

<script type="math/tex; mode=display">\Pr[w~\mbox{emitted at time $t$}~|~c_t] \propto \exp(c_t\cdot v_w). \qquad (2)</script>

<p>One imagines  there exists a “clothing” discourse that has high probability of outputting the <em>tie1</em> sense, and also of outputting related words such as <em>shoe, jacket,</em> etc.
Similarly there may be a  “games/matches” discourse that has high probability of outputting  <em>tie2</em> as well as <em>team, score</em> etc.</p>

<p>By equation (2) the probability of being output by a discourse is determined by the 
inner product, so one expects that the vector  for  “clothing” discourse  has high inner product with all of <em>shoe, jacket, tie1</em> etc., and thus can stand as surrogate for $v_{tie1}$ in expression (1)!  This motivates the following  global optimization:</p>

<blockquote>
  <p>Given word vectors in $\Re^d$, totaling  about $60,000$ in this case, a sparsity parameter $k$,
and an upper bound $m$, find a set of unit vectors   $A_1, A_2, \ldots, A_m$  such that
<script type="math/tex">v_w = \sum_{j=1}^m\alpha_{w,j}A_j + \eta_w \qquad (3)</script>
where at most $k$ of the coefficients $\alpha_{w,1},\dots,\alpha_{w,m}$ are nonzero (so-called  <em>hard sparsity constraint</em>), and $\eta_w$ is a  noise vector.</p>
</blockquote>

<p>Here  $A_1, \ldots A_m$ represent important discourses in the corpus, which 
we refer to as  <em>atoms of discourse.</em></p>

<p>Optimization (3) is a surrogate for the desired expansion of $v_{tie}$ in (1)  because one can hope that the atoms of discourse  will contain atoms corresponding to  <em>clothing</em>, <em>sports matches</em> etc. that will have high inner product (close to $1$) with <em>tie1,</em>  <em>tie2</em> respectively. Furthermore, restricting $m$ to be much smaller than the number of words ensures that each atom  needs to be used for multiple words, e.g., reuse the “clothing” atom 
for <em>shoes</em>, <em>jacket</em> etc. as well as for <em>tie</em>.</p>

<p>Both $A_j$’s and $\alpha_{w,j}$’s are unknowns in this optimization. This is nothing but <em>sparse coding,</em>  useful in neuroscience, image processing, computer vision,  etc. It is nonconvex and computationally NP-hard in the worst case, but can be solved quite efficiently in practice  using something called the k-SVD algorithm described in <a href="http://www.cs.technion.ac.il/~elad/publications/others/PCMI2010-Elad.pdf">Elad’s survey, lecture 4</a>.  We solved this problem with sparsity
$k=5$ and  using $m$ about $2000$. (Experimental details are in the paper. Also, some theoretical
analysis of such an algorithm is possible; see this <a href="http://www.offconvex.org/2016/05/08/almostconvexitySATM/">earlier post</a>.)</p>

<h1 id="experimental-results">Experimental Results</h1>

<p>Each discourse atom defines via (2) a distribution on words, which due to the exponential appearing in (2) strongly favors words whose embeddings have a larger inner product with it. In practice, this distribution is quite concentrated on as few as  50-100 words, and the “meaning” of a discourse atom can be roughly determined by looking at a few nearby words. This is how we visualize atoms in the figures below. The first figure gives a few representative atoms of discourse.</p>

<p style="text-align:center;">
<img src="http://www.cs.princeton.edu/~arora/pubs/discourseatoms.jpg" alt="A few of the 2000 atoms of discourse found" />
</p>

<p>And here are the discourse atoms used to represent two polysemous words, <em>tie</em> and <em>spring</em></p>

<p style="text-align:center;">
<img src="http://www.cs.princeton.edu/~arora/pubs/atomspolysemy.jpg" alt="Discourse atoms expressing the words tie and spring." />
</p>

<p>You can see that the discourse atoms do correspond to senses of these words.</p>

<p>Finally, we also have a technique that, given a target word, generates representative sentences according to its various senses as detected by the algorithm. Below are the sentences returned for
<em>ring.</em> (N.B. The mathematical meaning was missing in WordNet but was picked up by our method.)</p>
<p style="text-align:center;">
<img src="http://www.cs.princeton.edu/~arora/pubs/repsentences.jpg" alt="Representative sentences for different senses of the word ring." />
</p>

<h2 id="a-new-testbed-for-testing-comprehension-of-word-senses">A new testbed for testing comprehension of word senses</h2>

<p>Many tests have been proposed to test an algorithm’s grasp of word senses. They often involve
hard-to-understand metrics such as  distance in WordNet, or sometimes  tied to performance on specific applications like web search.</p>

<p>We propose a new simple test –inspired by word-intrusion tests for topic coherence
due to <a href="https://www.umiacs.umd.edu/~jbg/docs/nips2009-rtl.pdf">Chang et al 2009</a>– which has the advantages of being easy to understand, and can also be administered to humans.</p>

<p>We created a testbed using 200 polysemous words and their 704 senses according to WordNet. Each “sense”  is represented by a set of 8 related words; these were collected from WordNet and online dictionaries by college students who were told  to identify  most relevant other words occurring in the online definitions of this word sense as well as in the accompanying illustrative sentences.  These 8 words  are considered as <em>ground truth</em> representation of the word sense: e.g., for the  “tool/weapon” sense of  <em>axe</em> they were:  <em>handle, harvest, cutting,  split, tool, wood, battle, chop.</em></p>

<blockquote>
  <p><strong>Police line-up test for word senses:</strong> the algorithm is given a random one of these 200 polysemous words and a set of $m$ senses which contain the true sense for the word as well as some <em>distractors,</em> which are randomly picked senses from other words in the testbed. The test taker has to identify the word’s true senses amont these $m$ senses.</p>
</blockquote>

<p>As usual, accuracy is measured using <em>precision</em> (what fraction of the algorithm/human’s guesses 
were correct) and <em>recall</em> (how many correct senses were among the guesses).</p>

<p>For $m=20$ and $k=4$, our algorithm succeeds with precision  $63\%$ and recall $70\%$, and performance remains reasonable for $m=50$. We also administered the test to a group of grad students.
Native English speakers had precision/recall scores in the $75$ to $90$ percent range. 
Non-native speakers had scores roughly similar to our algorithm.</p>

<p>Our algorithm works something like this: If $w$ is the target word, then take all discourse atoms 
computed for that word, and compute a certain similarity score between each atom and each of the $m$ senses, where the words in the senses are represented by their word vectors. (Details are in the paper.)</p>

<h2 id="takeaways">Takeaways</h2>

<p>Word embeddings have been useful in a host of other settings, and now it appears that 
they also can easily yield different senses of a polysemous word. We have some subsequent applications of these ideas to other previously studied settings, including topic models, creating 
WordNets for other languages,  and understanding the semantic content of fMRI brain measurements. I’ll describe some of them in future posts.</p>

</article>

<div class="generic-box">
Subscribe to our <a href="/feed.xml">RSS feed</a>.

  <div class="share-page">
<div class="share-links" style="text-align:center;">
  Spread the word:   
    
      <a class = "fa fa-facebook" href="https://facebook.com/sharer.php?u=http://localhost:4000/2016/07/10/embeddingspolysemy/" rel="nofollow" target="_blank" title="Share on Facebook"></a>
    

    
      <a class="fa fa-twitter" href="https://twitter.com/intent/tweet?text=Linear algebraic structure of word meanings&url=http://localhost:4000/2016/07/10/embeddingspolysemy/" rel="nofollow" target="_blank" title="Share on Twitter"></a>
    

    
      <a class="fa fa-google-plus" href="https://plus.google.com/share?url=http://localhost:4000/2016/07/10/embeddingspolysemy/" rel="nofollow" target="_blank" title="Share on Google+"></a>
    

    
      <a class="fa fa-linkedin" href="http://www.linkedin.com/shareArticle?url=http://localhost:4000/2016/07/10/embeddingspolysemy/&title=Linear algebraic structure of word meanings" rel="nofollow" target="_blank" title="Share on LinkedIn"></a>
    

    

    

    
      <a class="fa fa-reddit" href="http://reddit.com/submit?url=http://localhost:4000/2016/07/10/embeddingspolysemy/&title=Linear algebraic structure of word meanings" rel="nofollow" target="_blank" title="Share on Reddit"></a>
    

    

    
      <a class = "fa fa-hacker-news" onclick="parent.postMessage('submit','*')" href="https://news.ycombinator.com/submitlink?u=http://localhost:4000/2016/07/10/embeddingspolysemy/&t=Linear algebraic structure of word meanings" rel="nofollow" target="_blank" title="Share on Hacker News"></a>
    
  </div>
</div>


</div>




  <h1>Comments</h1>
  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname  = 'offconvex';
    var disqus_identifier = '/2016/07/10/embeddingspolysemy';
    var disqus_title      = 'Linear algebraic structure of word meanings';

    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




      </div>
    </div>
  </div>

  <footer class="center">
  <div class="measure">
    <small>
      Theme available on <a href="https://github.com/johnotander/pixyll">Github</a>.
    </small>
  </div>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-70478681-1', 'auto');
    ga('send', 'pageview');
  </script>
</footer>

  
  
</body>
</html>
